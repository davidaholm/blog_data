{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATR-406 Final Project\n",
    "## Prep ACS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw ACS data cannot be used as features directly.  This notebook extracts the raw variables from the files at the\n",
    "county level for use in following steps.  Data was manually downloaded from the links below, unzipped as necessary, and moved to the **data > raw** subfolder of this project.\n",
    "\n",
    "[Raw data zip](https://www2.census.gov/acs2005_2009_5yr/summaryfile/2005-2009_ACSSF_All_In_2_Giant_Files(Experienced-Users-Only)/All_Geographies_Not_Tracts_Block_Groups.zip)<br>\n",
    "[Sequence Number](https://www2.census.gov/acs2005_2009_5yr/summaryfile/Sequence_Number_and_Table_Number_Lookup.xls)<br>\n",
    "[Table shells](https://www2.census.gov/acs2005_2009_5yr/summaryfile/ACS2009_5-Year_TableShells.xls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER_FILE = './data/raw/ACS2009_5-Year_TableShells.xls'\n",
    "SEQUENCE_FILE = './data/raw/Sequence_Number_and_Table_Number_Lookup.xls'\n",
    "DATA_PATH = './data/raw/All_Geographies_Not_Tracts_Block_Groups/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prep Metadata\n",
    "The ACS data files do not include headers.  That information in stored in a separate file.  Here, I read and clean the header file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  table_id  sequence_number  line_number                             label  \\\n2   B07401                1            1  Total living in area 1 year ago:   \n3   B07401                1            2                      1 to 4 years   \n4   B07401                1            3                     5 to 17 years   \n5   B07401                1            4                   18 and 19 years   \n6   B07401                1            5                    20 to 24 years   \n\n                                         table_title  \\\n2  GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...   \n3  GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...   \n4  GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...   \n5  GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...   \n6  GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...   \n\n                                           universe    variable  \n2   Population 1 year and over in the United States  B07401_001  \n3   Population 1 year and over in the United States  B07401_002  \n4   Population 1 year and over in the United States  B07401_003  \n5   Population 1 year and over in the United States  B07401_004  \n6   Population 1 year and over in the United States  B07401_005  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>table_id</th>\n      <th>sequence_number</th>\n      <th>line_number</th>\n      <th>label</th>\n      <th>table_title</th>\n      <th>universe</th>\n      <th>variable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>B07401</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Total living in area 1 year ago:</td>\n      <td>GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...</td>\n      <td>Population 1 year and over in the United States</td>\n      <td>B07401_001</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B07401</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1 to 4 years</td>\n      <td>GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...</td>\n      <td>Population 1 year and over in the United States</td>\n      <td>B07401_002</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B07401</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5 to 17 years</td>\n      <td>GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...</td>\n      <td>Population 1 year and over in the United States</td>\n      <td>B07401_003</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>B07401</td>\n      <td>1</td>\n      <td>4</td>\n      <td>18 and 19 years</td>\n      <td>GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...</td>\n      <td>Population 1 year and over in the United States</td>\n      <td>B07401_004</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>B07401</td>\n      <td>1</td>\n      <td>5</td>\n      <td>20 to 24 years</td>\n      <td>GEOGRAPHICAL MOBILITY IN THE PAST YEAR BY AGE ...</td>\n      <td>Population 1 year and over in the United States</td>\n      <td>B07401_005</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "seq = pd.read_excel(SEQUENCE_FILE)\n",
    "seq.columns = [x.lower().replace('\\n', '_').replace(' ', '_') for x in seq.columns]\n",
    "seq.drop(['file_id', 'total_cells_in_sequence', 'subject_area'], axis='columns', inplace=True)\n",
    "seq.rename({'table_title': 'label'}, axis='columns', inplace=True)\n",
    "\n",
    "seq['table_title'] = ''\n",
    "seq['universe'] = ''\n",
    "\n",
    "table_group = seq.groupby('table_id')\n",
    "for name, group in table_group:\n",
    "    cur_title = group['label'].values[0]\n",
    "    cur_universe = re.sub('^Universe: ', '', group['label'].values[1])\n",
    "    seq.loc[seq['table_id'] == name, 'table_title'] = cur_title\n",
    "    seq.loc[seq['table_id'] == name, 'universe'] = cur_universe\n",
    "    \n",
    "seq = seq.loc[seq['line_number'].notnull(), :]\n",
    "seq = seq.loc[seq['line_number'] != 0.5, :]\n",
    "seq.drop(['start_position', 'total_cells_in_table'], axis='columns', inplace=True)\n",
    "\n",
    "seq['line_number'] = seq['line_number'].astype(int)\n",
    "seq['variable'] = seq['table_id'] + '_' + seq['line_number'].astype(str).str.zfill(3)\n",
    "\n",
    "seq.to_csv('data/sequence_info.csv', index=False)\n",
    "\n",
    "display(seq.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                         table_title    variable  num_groups\n0  AGE AND NATIVITY OF OWN CHILDREN UNDER 18 YEAR...  B05009_001           1\n1  AGE BY LANGUAGE SPOKEN AT HOME BY ABILITY TO S...  B16004_001           1\n2  AGE BY LANGUAGE SPOKEN AT HOME FOR THE POPULAT...  B16007_001           1\n3  AGE BY LANGUAGE SPOKEN AT HOME FOR THE POPULAT...  B16003_001           1\n4  AGE BY RATIO OF INCOME TO POVERTY LEVEL IN THE...  B17024_001           1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>table_title</th>\n      <th>variable</th>\n      <th>num_groups</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AGE AND NATIVITY OF OWN CHILDREN UNDER 18 YEAR...</td>\n      <td>B05009_001</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AGE BY LANGUAGE SPOKEN AT HOME BY ABILITY TO S...</td>\n      <td>B16004_001</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AGE BY LANGUAGE SPOKEN AT HOME FOR THE POPULAT...</td>\n      <td>B16007_001</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AGE BY LANGUAGE SPOKEN AT HOME FOR THE POPULAT...</td>\n      <td>B16003_001</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AGE BY RATIO OF INCOME TO POVERTY LEVEL IN THE...</td>\n      <td>B17024_001</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# grouping by table name to make it easier to find variables of interest\n",
    "table_meta = seq[['table_title', 'variable']].groupby('table_title').min()\n",
    "table_meta = table_meta.reset_index()\n",
    "table_meta['num_groups'] = table_meta['variable'].str.split(' BY ').apply(len)\n",
    "table_meta.sort_values(['num_groups', 'table_title'], inplace=True)\n",
    "table_meta.to_csv('data/table_names.csv', index=False)\n",
    "\n",
    "table_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored with logrecno (record number) as the unique identifier.  Each state folder contains a lookup between\n",
    "logrecno and the components needed to construct geoid.  The header for these lookup files can be found [in this pdf](https://www2.census.gov/acs2005_2009_5yr/summaryfile/ACS_2005-2009_SF_Tech_Doc.pdf), transcribed manually here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "51 cols, 51 widths\n"
    }
   ],
   "source": [
    "GEO_COLS = ['fileid', 'stusab', 'sumlevel', 'component', 'logrecno', 'us', 'region', 'division', 'statece', \n",
    "            'state', 'county', 'cousub', 'place', 'tract', 'blkgrp', 'concit', 'aianhh', 'aianhhfp', 'aihhtli', \n",
    "            'aitsce', 'aits', 'anrc', 'cbsa', 'csa', 'metdiv', 'macc', 'memi', 'necta', 'cnecta', 'nectadiv', \n",
    "            'ua', 'blank_1', 'cdcurr', 'sldu', 'sldl', 'blank_2', 'blank_3', 'blank_4', 'submcd', 'sdelm', \n",
    "            'sdsec', 'sduni', 'ur', 'pci', 'blank_5', 'blank_6', 'puma5', 'blank_7', 'geoid', 'name', 'blank_8']\n",
    "\n",
    "GEO_WIDTHS = [6, 2, 3, 2, 7, 1, 1, 1, 2, 2, 3, 5, 5, 6, 1, 5, 4, 5, 1, 3, 5, 5, 5, 3, 5, 1, 1, 5, 3, 5, 5, 5, 2, 3,\n",
    "             3, 6, 3, 5, 5, 5, 5, 5, 1, 1, 6, 5, 5, 5, 40, 200, 50]\n",
    "\n",
    "print('{} cols, {} widths'.format(len(GEO_COLS), len(GEO_WIDTHS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prep Variables to Extract\n",
    "The list of variables, with labels, are stored in acs_variables.txt.  There are over 20,000 variables in these ACS data.  I have selected a number of demographic variables based on my experience but it by no means an exhaustive list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "202 ACS variables to download\n"
    }
   ],
   "source": [
    "acs_vars_raw = pd.read_csv('data/acs_variables.txt', sep='\\t')\n",
    "acs_vars = acs_vars_raw['variable'].tolist()\n",
    "print('{} ACS variables to download'.format(len(acs_vars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract Data by State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "52 states, including DC and Puerto Rico\n"
    }
   ],
   "source": [
    "# get list of states\n",
    "states = os.listdir(DATA_PATH)\n",
    "states = [x.split('_')[0] for x in states if '.zip' in x]\n",
    "states = [x for x in states if x != 'UnitedStates']\n",
    "states.sort()\n",
    "print('{} states, including DC and Puerto Rico'.format(len(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lookup from files to variables\n",
    "files_to_vars = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "for v in acs_vars:\n",
    "    file_num = seq.loc[seq['variable'] == v, 'sequence_number']\n",
    "    if file_num.shape[0] > 0:\n",
    "        file_num = file_num.values[0]\n",
    "        file_name = 'e20095XX{num}.txt'.format(num='{:07d}'.format(file_num * 1000))\n",
    "        file_name = file_name.replace('XX', '{st}')\n",
    "        files_to_vars[file_name]['cols'].append(v)\n",
    "        files_to_vars[file_name]['seq_num'] = file_num\n",
    "    else:\n",
    "        print('variable {} not found'.format(v))\n",
    "    \n",
    "# update dict with full list of cols/header for each file\n",
    "for k in files_to_vars.keys():\n",
    "    header = seq.loc[seq['sequence_number'] == files_to_vars[k]['seq_num'], 'variable'].values.tolist()\n",
    "    files_to_vars[k]['header'] = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  state  logrecno B00001_001 B00002_001  B01001_001  B01001_002  B01001_003  \\\n0    al        13       4560       1872     49584.0     24057.0      1683.0   \n1    al        14      10722       5295    171997.0     84263.0      5543.0   \n2    al        15       2479       1102     29663.0     15687.0       963.0   \n3    al        16       1348        579     21464.0     11164.0       614.0   \n4    al        17       4389       1875     56804.0     28216.0      1857.0   \n\n   B01001_004  B01001_005  B01001_006  ...  B25035_001  B24081_001  \\\n0      1824.0      2222.0      1327.0  ...        1987       31559   \n1      5251.0      6053.0      3725.0  ...        1992       28947   \n2       808.0      1083.0       679.0  ...        1980       24864   \n3       673.0       875.0       538.0  ...        1980       29348   \n4      1951.0      2090.0      1328.0  ...        1983       29382   \n\n   B24081_002  B24081_005  B24081_006  B24081_007  B24081_008  B24081_009  \\\n0       29975       28333       35777       38866       54679       20568   \n1       27796       28983       35826       36696       54868       25000   \n2       24404       18523       30756       37561       31806       13608   \n3       28004       31774       27428       46250       23750       35786   \n4       29299       28696       32500       32886       56087       20845   \n\n    geoid                     name  \n0  _01001  Autauga County, Alabama  \n1  _01003  Baldwin County, Alabama  \n2  _01005  Barbour County, Alabama  \n3  _01007     Bibb County, Alabama  \n4  _01009   Blount County, Alabama  \n\n[5 rows x 206 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state</th>\n      <th>logrecno</th>\n      <th>B00001_001</th>\n      <th>B00002_001</th>\n      <th>B01001_001</th>\n      <th>B01001_002</th>\n      <th>B01001_003</th>\n      <th>B01001_004</th>\n      <th>B01001_005</th>\n      <th>B01001_006</th>\n      <th>...</th>\n      <th>B25035_001</th>\n      <th>B24081_001</th>\n      <th>B24081_002</th>\n      <th>B24081_005</th>\n      <th>B24081_006</th>\n      <th>B24081_007</th>\n      <th>B24081_008</th>\n      <th>B24081_009</th>\n      <th>geoid</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>al</td>\n      <td>13</td>\n      <td>4560</td>\n      <td>1872</td>\n      <td>49584.0</td>\n      <td>24057.0</td>\n      <td>1683.0</td>\n      <td>1824.0</td>\n      <td>2222.0</td>\n      <td>1327.0</td>\n      <td>...</td>\n      <td>1987</td>\n      <td>31559</td>\n      <td>29975</td>\n      <td>28333</td>\n      <td>35777</td>\n      <td>38866</td>\n      <td>54679</td>\n      <td>20568</td>\n      <td>_01001</td>\n      <td>Autauga County, Alabama</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>al</td>\n      <td>14</td>\n      <td>10722</td>\n      <td>5295</td>\n      <td>171997.0</td>\n      <td>84263.0</td>\n      <td>5543.0</td>\n      <td>5251.0</td>\n      <td>6053.0</td>\n      <td>3725.0</td>\n      <td>...</td>\n      <td>1992</td>\n      <td>28947</td>\n      <td>27796</td>\n      <td>28983</td>\n      <td>35826</td>\n      <td>36696</td>\n      <td>54868</td>\n      <td>25000</td>\n      <td>_01003</td>\n      <td>Baldwin County, Alabama</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>al</td>\n      <td>15</td>\n      <td>2479</td>\n      <td>1102</td>\n      <td>29663.0</td>\n      <td>15687.0</td>\n      <td>963.0</td>\n      <td>808.0</td>\n      <td>1083.0</td>\n      <td>679.0</td>\n      <td>...</td>\n      <td>1980</td>\n      <td>24864</td>\n      <td>24404</td>\n      <td>18523</td>\n      <td>30756</td>\n      <td>37561</td>\n      <td>31806</td>\n      <td>13608</td>\n      <td>_01005</td>\n      <td>Barbour County, Alabama</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>al</td>\n      <td>16</td>\n      <td>1348</td>\n      <td>579</td>\n      <td>21464.0</td>\n      <td>11164.0</td>\n      <td>614.0</td>\n      <td>673.0</td>\n      <td>875.0</td>\n      <td>538.0</td>\n      <td>...</td>\n      <td>1980</td>\n      <td>29348</td>\n      <td>28004</td>\n      <td>31774</td>\n      <td>27428</td>\n      <td>46250</td>\n      <td>23750</td>\n      <td>35786</td>\n      <td>_01007</td>\n      <td>Bibb County, Alabama</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>al</td>\n      <td>17</td>\n      <td>4389</td>\n      <td>1875</td>\n      <td>56804.0</td>\n      <td>28216.0</td>\n      <td>1857.0</td>\n      <td>1951.0</td>\n      <td>2090.0</td>\n      <td>1328.0</td>\n      <td>...</td>\n      <td>1983</td>\n      <td>29382</td>\n      <td>29299</td>\n      <td>28696</td>\n      <td>32500</td>\n      <td>32886</td>\n      <td>56087</td>\n      <td>20845</td>\n      <td>_01009</td>\n      <td>Blount County, Alabama</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 206 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# load raw data by state\n",
    "std_cols = ['ignore1', 'ignore2', 'state', 'ignore3', 'ignore4', 'logrecno']\n",
    "\n",
    "acs_raw = pd.DataFrame()\n",
    "\n",
    "for state in states:\n",
    "    \n",
    "    state_df = pd.DataFrame()\n",
    "    \n",
    "    # unzip, if needed\n",
    "    if not os.path.exists(DATA_PATH + state):\n",
    "            zip_filename = '{}_All_Geographies_Not_Tracts_Block_Groups.zip'.format(state)\n",
    "            with zipfile.ZipFile(DATA_PATH + zip_filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(DATA_PATH + state)\n",
    "\n",
    "    # get abbreviation from file names\n",
    "    data_file = os.listdir(DATA_PATH + state)\n",
    "    data_file = [x for x in data_file if re.match('e20095..0001000.txt', x)]\n",
    "    state_abbr = re.sub('e20095(..)0001000.txt', '\\\\1', data_file[0])\n",
    "                \n",
    "    # build lookup between logrecno and geoid\n",
    "    geo_file = '{dp}{state}/g20095{st}.txt'.format(dp=DATA_PATH, state=state, st=state_abbr)\n",
    "    geo_lu = pd.read_fwf(geo_file, names=GEO_COLS, widths=GEO_WIDTHS)\n",
    "    geo_lu = geo_lu.loc[geo_lu['geoid'].str.match('05000US\\d{5}'), ['logrecno', 'geoid', 'name']]\n",
    "    geo_lu['geoid'] = geo_lu['geoid'].str.replace('^\\d+US', '_')\n",
    "    \n",
    "    # loop through files to pull out necessary vars\n",
    "    for f_raw in files_to_vars.keys():\n",
    "        f_name = '{dp}{state}/{f}'.format(dp=DATA_PATH, state=state, f=f_raw.format(st=state_abbr))\n",
    "        all_cols = std_cols + files_to_vars[f_raw]['header']\n",
    "        temp_df = pd.read_csv(f_name, names=all_cols, encoding='latin-1', low_memory=False)\n",
    "        keep_cols = ['state', 'logrecno'] + files_to_vars[f_raw]['cols']\n",
    "        temp_df = temp_df[keep_cols].copy()\n",
    "        if state_df.shape[0] > 0:\n",
    "            state_df = pd.merge(state_df, temp_df)\n",
    "        else:\n",
    "            state_df = temp_df.copy()\n",
    "    \n",
    "    # merge to get geoid's\n",
    "    state_df = pd.merge(state_df, geo_lu)\n",
    "    \n",
    "    acs_raw = acs_raw.append(state_df)\n",
    "\n",
    "acs_raw = acs_raw.reset_index(drop=True)\n",
    "acs_raw.to_csv('data/acs_raw.csv', index=False)\n",
    "display(acs_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this cell to build the ACS features (in the following cells) without rebuilding the raw data\n",
    "# acs_raw = pd.read_csv('data/acs_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_features_cols = ['state', 'geoid', 'name']\n",
    "acs_features = acs_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totals\n",
    "acs_features['total_pop'] = acs_features['B00001_001']\n",
    "acs_features['total_hholds'] = acs_features['B00002_001']\n",
    "acs_features_cols.extend(['total_pop', 'total_hholds'])\n",
    "\n",
    "# sex by age\n",
    "temp_cols = ['sexbyage_m_u_18', 'sexbyage_m_18_29', 'sexbyage_m_30_39', 'sexbyage_m_40_49', 'sexbyage_m_50_64', 'sexbyage_m_65_plus', 'sexbyage_f_u_18', 'sexbyage_f_18_29', 'sexbyage_f_30_39', 'sexbyage_f_40_49', 'sexbyage_f_50_64', 'sexbyage_f_65_plus']\n",
    "\n",
    "acs_features['sexbyage_m_u_18'] = acs_features['B01001_003'] + acs_features['B01001_004'] + acs_features['B01001_005'] + acs_features['B01001_006']\n",
    "acs_features['sexbyage_m_18_29'] = acs_features['B01001_007'] + acs_features['B01001_008'] + acs_features['B01001_009'] + acs_features['B01001_010'] + acs_features['B01001_011']\n",
    "acs_features['sexbyage_m_30_39'] = acs_features['B01001_012'] + acs_features['B01001_013']\n",
    "acs_features['sexbyage_m_40_49'] = acs_features['B01001_014'] + acs_features['B01001_015']\n",
    "acs_features['sexbyage_m_50_64'] = acs_features['B01001_016'] + acs_features['B01001_017'] + acs_features['B01001_018'] + acs_features['B01001_019']\n",
    "acs_features['sexbyage_m_65_plus'] = acs_features['B01001_020'] + acs_features['B01001_021'] + acs_features['B01001_022'] + acs_features['B01001_023'] + acs_features['B01001_024'] + acs_features['B01001_025']\n",
    "\n",
    "acs_features['sexbyage_f_u_18'] = acs_features['B01001_027'] + acs_features['B01001_028'] + acs_features['B01001_029'] + acs_features['B01001_030']\n",
    "acs_features['sexbyage_f_18_29'] = acs_features['B01001_031'] + acs_features['B01001_032'] + acs_features['B01001_033'] + acs_features['B01001_034'] + acs_features['B01001_035']\n",
    "acs_features['sexbyage_f_30_39'] = acs_features['B01001_036'] + acs_features['B01001_037']\n",
    "acs_features['sexbyage_f_40_49'] = acs_features['B01001_038'] + acs_features['B01001_039']\n",
    "acs_features['sexbyage_f_50_64'] = acs_features['B01001_040'] + acs_features['B01001_041'] + acs_features['B01001_042'] + acs_features['B01001_043']\n",
    "acs_features['sexbyage_f_65_plus'] = acs_features['B01001_044'] + acs_features['B01001_045'] + acs_features['B01001_046'] + acs_features['B01001_047'] + acs_features['B01001_048'] + acs_features['B01001_049']\n",
    "\n",
    "for c in temp_cols:\n",
    "    acs_features[c] = acs_features[c] / acs_features['B01001_001']\n",
    "\n",
    "# language\n",
    "acs_features['speak_only_english'] = acs_features['B06007_002'] / acs_features['B06007_001']\n",
    "acs_features_cols.append('speak_only_english')\n",
    "\n",
    "# educational attainment\n",
    "acs_features['educ_less_than_hs'] = acs_features['B06009_002'] / acs_features['B06009_001']\n",
    "acs_features['educ_hs_grad'] = acs_features['B06009_003'] / acs_features['B06009_001']\n",
    "acs_features['educ_some_college'] = acs_features['B06009_004'] / acs_features['B06009_001']\n",
    "acs_features['educ_college_grad'] = acs_features['B06009_005'] / acs_features['B06009_001']\n",
    "acs_features['educ_post_grad'] = acs_features['B06009_006'] / acs_features['B06009_001']\n",
    "acs_features_cols.extend(['educ_less_than_hs', 'educ_hs_grad', 'educ_some_college', 'educ_college_grad', 'educ_post_grad'])\n",
    "\n",
    "# income\n",
    "acs_features['income_u10k'] = acs_features['B06010_004'] / acs_features['B06010_001']\n",
    "acs_features['income_10k_to_u15k'] = acs_features['B06010_005'] / acs_features['B06010_001']\n",
    "acs_features['income_15k_to_u25k'] = acs_features['B06010_006'] / acs_features['B06010_001']\n",
    "acs_features['income_25k_to_u35k'] = acs_features['B06010_007'] / acs_features['B06010_001']\n",
    "acs_features['income_35k_to_u50k'] = acs_features['B06010_008'] / acs_features['B06010_001']\n",
    "acs_features['income_50k_to_u65k'] = acs_features['B06010_009'] / acs_features['B06010_001']\n",
    "acs_features['income_65k_to_u75k'] = acs_features['B06010_010'] / acs_features['B06010_001']\n",
    "acs_features['income_75k_or_more'] = acs_features['B06010_011'] / acs_features['B06010_001']\n",
    "acs_features_cols.extend(['income_u10k', 'income_10k_to_u15k', 'income_15k_to_u25k', 'income_25k_to_u35k', 'income_35k_to_u50k', 'income_50k_to_u65k', 'income_65k_to_u75k', 'income_75k_or_more'])\n",
    "\n",
    "# marital status -- excluding \"married\"\n",
    "acs_features['marital_never_married'] = acs_features['B06008_002'] / acs_features['B06008_001']\n",
    "acs_features_cols.append('marital_never_married')\n",
    "\n",
    "# geographic mobility\n",
    "bcols = [x for x in acs_features.columns if re.match('^B07401_.*', x)]\n",
    "for c in bcols:\n",
    "    acs_features[c] = acs_features[c].astype(str).str.replace('^\\.+$', '0').astype(float)\n",
    "\n",
    "acs_features['livinginarea_1_to_4'] = acs_features['B07401_002']\n",
    "acs_features['livinginarea_5_to_17'] = acs_features['B07401_003']\n",
    "acs_features['livinginarea_18_to_19'] = acs_features['B07401_004']\n",
    "acs_features['livinginarea_20_to_49'] = acs_features['B07401_005'] + acs_features['B07401_006'] + acs_features['B07401_007'] + acs_features['B07401_008'] + acs_features['B07401_009'] + acs_features['B07401_010']\n",
    "acs_features['livinginarea_50_plus'] = acs_features['B07401_011'] + acs_features['B07401_012'] + acs_features['B07401_013'] + acs_features['B07401_014'] + acs_features['B07401_015'] + acs_features['B07401_016']\n",
    "temp_cols = [x for x in acs_features.columns if re.match('^livinginarea.*', x)]\n",
    "\n",
    "for c in temp_cols:\n",
    "    acs_features[c] = acs_features[c] / acs_features['B07401_001']\n",
    "acs_features_cols.extend(temp_cols)\n",
    "\n",
    "# citizenship -- excluding \"citizen\"\n",
    "acs_features['citizenship_non_citizen'] = acs_features['B05001_006'] / acs_features['B05001_001']\n",
    "acs_features_cols.append('citizenship_non_citizen')\n",
    "\n",
    "# income vs poverty level\n",
    "acs_features['incomepovertyratio_u1'] = acs_features['B05010_002']\n",
    "acs_features['incomepovertyratio_1_to_2'] = acs_features['B05010_010']\n",
    "acs_features['incomepovertyratio_2_plus'] = acs_features['B05010_018']\n",
    "temp_cols = [x for x in acs_features.columns if re.match('^incomepovertyratio.*', x)]\n",
    "for c in temp_cols:\n",
    "    acs_features[c] = acs_features[c] / acs_features['B05010_001']\n",
    "acs_features_cols.extend(temp_cols)\n",
    "\n",
    "# household earnings -- excluding \"households without earnings\"\n",
    "acs_features['hh_with_earnings'] = acs_features['B19051_002'] / acs_features['B19051_001']\n",
    "acs_features_cols.append('hh_with_earnings')\n",
    "\n",
    "# employment -- excluding \"not in labor force\"\n",
    "temp_cols = ['employ_employed', 'employ_unemployed']\n",
    "acs_features['employ_employed'] = acs_features['B12006_005'] + acs_features['B12006_010'] + acs_features['B12006_016'] + acs_features['B12006_021'] + acs_features['B12006_027'] + acs_features['B12006_032'] + acs_features['B12006_038'] + acs_features['B12006_043'] + acs_features['B12006_049'] + acs_features['B12006_054']\n",
    "acs_features['employ_unemployed'] = acs_features['B12006_006'] + acs_features['B12006_011'] + acs_features['B12006_017'] + acs_features['B12006_022'] + acs_features['B12006_028'] + acs_features['B12006_033'] + acs_features['B12006_039'] + acs_features['B12006_044'] + acs_features['B12006_050'] + acs_features['B12006_055']\n",
    "\n",
    "for c in temp_cols:\n",
    "    acs_features[c] = acs_features[c] / acs_features['B12006_001']\n",
    "acs_features_cols.extend(temp_cols)\n",
    "\n",
    "# gini index of income inequality\n",
    "acs_features['inequality_index_gini'] = acs_features['B19083_001']\n",
    "acs_features_cols.append('inequality_index_gini')\n",
    "\n",
    "# household type\n",
    "acs_features['hhtype_married_couple'] = acs_features['B11001_003'] / acs_features['B11001_001']\n",
    "acs_features['hhtype_m_householder'] = acs_features['B11001_005'] / acs_features['B11001_001']\n",
    "acs_features['hhtype_f_householder'] = acs_features['B11001_006'] / acs_features['B11001_001']\n",
    "acs_features['hhtype_living_alone'] = acs_features['B11001_008'] / acs_features['B11001_001']\n",
    "acs_features_cols.extend(['hhtype_married_couple', 'hhtype_m_householder', 'hhtype_f_householder', 'hhtype_living_alone'])\n",
    "\n",
    "# transportation to work -- excluding other (taxiab, motorcycle, other means)\n",
    "temp_cols = ['traveltowork_drove_alone', 'traveltowork_carpooled', 'traveltowork_public_transit']\n",
    "acs_features['traveltowork_drove_alone'] = acs_features['B08301_003']\n",
    "acs_features['traveltowork_carpooled'] = acs_features['B08301_004']\n",
    "acs_features['traveltowork_public_transit'] = acs_features['B08301_010']\n",
    "acs_features['traveltowork_bike_walk'] = acs_features['B08301_018'] + acs_features['B08301_019']\n",
    "acs_features['traveltowork_work_at_home'] = acs_features['B08301_021']\n",
    "\n",
    "for c in temp_cols:\n",
    "    acs_features[c] = acs_features[c] / acs_features['B08301_001']\n",
    "acs_features_cols.extend(temp_cols)\n",
    "\n",
    "# length of commute to work\n",
    "temp_cols = ['commutetime_u10', 'commutetime_10_30', 'commutetime_30_60', 'commute_time_60_90', 'commutetime_90_plus']\n",
    "acs_features['commutetime_u10'] = acs_features['B08303_002'] + acs_features['B08303_003']\n",
    "acs_features['commutetime_10_30'] = acs_features['B08303_004'] + acs_features['B08303_005'] + acs_features['B08303_006'] + acs_features['B08303_007']\n",
    "acs_features['commutetime_30_60'] = acs_features['B08303_008'] + acs_features['B08303_009'] + acs_features['B08303_010'] + acs_features['B08303_011']\n",
    "acs_features['commute_time_60_90'] = acs_features['B08303_012']\n",
    "acs_features['commutetime_90_plus'] = acs_features['B08303_013']\n",
    "\n",
    "for c in temp_cols:\n",
    "    acs_features[c] = acs_features[c] / acs_features['B08303_001']\n",
    "acs_features_cols.extend(temp_cols)\n",
    "\n",
    "# median year structure built\n",
    "acs_features['structure_year_built_median'] = acs_features['B25035_001'].str.replace('^\\.+$', '0').astype(float)\n",
    "acs_features_cols.append('structure_year_built_median')\n",
    "\n",
    "# work type -- excluding \"self-employed\"\n",
    "bcols = [x for x in acs_features.columns if re.match('^B24081_.*', x)]\n",
    "for c in bcols:\n",
    "    acs_features[c] = acs_features[c].astype(str).str.replace('^\\.+$', '0').astype(float)\n",
    "\n",
    "temp_cols = ['worktype_for_profit', 'worktype_non_profit', 'worktype_government']\n",
    "acs_features['worktype_for_profit'] = acs_features['B24081_002']\n",
    "acs_features['worktype_non_profit'] = acs_features['B24081_005']\n",
    "acs_features['worktype_government'] = acs_features['B24081_006'] + acs_features['B24081_007'] + acs_features['B24081_008']\n",
    "\n",
    "for c in temp_cols:\n",
    "    acs_features[c] = acs_features[c] / acs_features['B24081_001']\n",
    "acs_features_cols.extend(temp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3221, 49)\n"
    }
   ],
   "source": [
    "# select feature cols and output to file\n",
    "acs_features = acs_features[acs_features_cols]\n",
    "acs_features['state'] = acs_features['state'].str.upper()\n",
    "\n",
    "acs_features.to_csv('data/acs_features.csv', index=False)\n",
    "print(acs_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}